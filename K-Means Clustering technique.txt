#K-Means Clustering technique
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# -----------------------------
# 1. Load and prepare data
# -----------------------------
iris = load_iris()
X = iris.data[:, :2]  # take first two features for easy 2D visualization

scaler = StandardScaler()
X = scaler.fit_transform(X)

# -----------------------------
# 2. Elbow Method (to find optimal K)
# -----------------------------
inertias = []
K = range(1, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)  # sum of squared distances to nearest cluster center

x1, y1 = 1, inertias[0]
x2, y2 = max(K), inertias[-1]

# distances of all points from line
distances = []
for i in range(len(K)):
    x0, y0 = K[i], inertias[i]
    num = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2*y1 - y2*x1)
    den = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)
    distances.append(num / den)

k_final = K[np.argmax(distances)]
print(f"\nâœ… Finalized Optimal k value (from Elbow Method): {k_final}")

plt.figure(figsize=(6, 4))
plt.plot(K, inertias, marker='o')
plt.title("Elbow Method for Optimal k")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (Within-Cluster Sum of Squares)")
plt.grid(True)
plt.show()

# -----------------------------
# 3. K-Means Clustering (k=3)
# -----------------------------
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X)

# Plot final clusters
plt.figure(figsize=(7, 5))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', marker='X', label='Centroids')
plt.title("Final K-Means Clusters (k=3)")
plt.xlabel("Sepal Length (normalized)")
plt.ylabel("Sepal Width (normalized)")
plt.legend()
plt.show()